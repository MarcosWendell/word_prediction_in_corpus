{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from copy import deepcopy\n",
    "from collections import Counter, defaultdict, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/Users/giovannicassani/Desktop/CL/LanguageModel/training_corpus.json'\n",
    "test_path = '/Users/giovannicassani/Desktop/CL/LanguageModel/test_corpus.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    This class creates a corpus object read off a .json file consisting of a list of lists,\n",
    "    where each inner list is a sentence encoded as a list of strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, path, n, t, bos_eos=True, vocab=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        A Corpus object has the following attributes:\n",
    "         - vocab: set or None (default). If a set is passed, words in the input file not \n",
    "                         found in the set are replaced with the UNK string\n",
    "         - path: str, the path to the .json file used to build the corpus object\n",
    "         - t: int, words with frequency count < t are replaced with the UNK string\n",
    "         - ngram_size: int, 2 for bigrams, 3 for trigrams, and so on.\n",
    "         - bos_eos: bool, default to True. If False, bos and eos symbols are not \n",
    "                     prepended and appended to sentences.\n",
    "         - sentences: list of lists, containing the input sentences after lowercasing and \n",
    "                         splitting at the white space\n",
    "         - frequencies: Counter, mapping tokens to their frequency count in the corpus\n",
    "        \"\"\"\n",
    "        \n",
    "        self.vocab = vocab        \n",
    "        self.path = path\n",
    "        self.ngram_size = n\n",
    "        self.t = t\n",
    "        self.bos_eos = bos_eos\n",
    "        \n",
    "        self.sentences = self.read()\n",
    "        # output --> [['i', 'am', 'home' '.'], ['you', 'went', 'to', 'the', 'park', '.'], ...]\n",
    "    \n",
    "        self.frequencies = self.freq_distr()\n",
    "        # output --> Counter('the': 485099, 'of': 301877, 'i': 286549, ...)\n",
    "        # the numbers are made up, they aren't the actual frequency counts\n",
    "        \n",
    "        if self.t or self.vocab:\n",
    "            # input --> [['i', 'am', 'home' '.'], ['you', 'went', 'to', 'the', 'park', '.'], ...]\n",
    "            self.sentences = self.filter_words()\n",
    "            # output --> [['i', 'am', 'home' '.'], ['you', 'went', 'to', 'the', 'UNK', '.'], ...]\n",
    "            # supposing that park wasn't frequent enough or was outside of the training \n",
    "            # vocabulary, it gets replaced by the UNK string\n",
    "            \n",
    "        if self.bos_eos:\n",
    "            # input --> [['i', 'am', 'home' '.'], ...]\n",
    "            self.sentences = self.add_bos_eos()\n",
    "            # output --> [['bos', 'i', 'am', 'home', '.', 'eos'], ...]\n",
    "                    \n",
    "    def read(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Reads the sentences off the .json file, replaces quotes, lowercases strings and splits \n",
    "        at the white space. Returns a list of lists.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.path.endswith('.json'):\n",
    "            sentences = json.load(open(self.path, 'r'))                \n",
    "        else:   \n",
    "            sentences = []\n",
    "            with open(self.path, 'r', encoding='latin-1') as f:\n",
    "                for line in f:\n",
    "                    print(line[:20])\n",
    "                    # first strip away newline symbols and the like, then replace ' and \" with the empty \n",
    "                    # string and get rid of possible remaining trailing spaces \n",
    "                    line = line.strip().translate({ord(i): None for i in '\"\\'\\\\'}).strip(' ')\n",
    "                    # lowercase and split at the white space (the corpus has ben previously tokenized)\n",
    "                    sentences.append(line.lower().split(' '))\n",
    "        \n",
    "        return sentences\n",
    "    \n",
    "    def freq_distr(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Creates a counter mapping tokens to frequency counts\n",
    "        \n",
    "        count = Counter()\n",
    "        for sentence in self.sentences:\n",
    "            for word in sentence:\n",
    "                count[w] += 1\n",
    "        \"\"\"\n",
    "    \n",
    "        return Counter([word for sentence in self.sentences for word in sentence])\n",
    "        \n",
    "    \n",
    "    def filter_words(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Replaces illegal tokens with the UNK string. A token is illegal if its frequency count\n",
    "        is lower than the given threshold and/or if it falls outside the specified vocabulary.\n",
    "        The two filters can be both active at the same time but don't have to be. To exclude the \n",
    "        frequency filter, set t=0 in the class call.\n",
    "        \"\"\"\n",
    "                \n",
    "        filtered_sentences = []\n",
    "        for sentence in self.sentences:\n",
    "            filtered_sentence = []\n",
    "            for word in sentence:\n",
    "                if self.t and self.vocab:\n",
    "                    # check that the word is frequent enough and occurs in the vocabulary\n",
    "                    filtered_sentence.append(\n",
    "                        word if self.frequencies[word] > self.t and word in self.vocab else 'UNK'\n",
    "                    )\n",
    "                else:\n",
    "                    if self.t:\n",
    "                        # check that the word is frequent enough\n",
    "                        filtered_sentence.append(word if self.frequencies[word] > self.t else 'UNK')\n",
    "                    else:\n",
    "                        # check if the word occurs in the vocabulary\n",
    "                        filtered_sentence.append(word if word in self.vocab else 'UNK')\n",
    "                        \n",
    "            if len(filtered_sentence) > 1:\n",
    "                # make sure that the sentence contains more than 1 token\n",
    "                filtered_sentences.append(filtered_sentence)\n",
    "    \n",
    "        return filtered_sentences\n",
    "    \n",
    "    def add_bos_eos(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Adds the necessary number of BOS symbols and one EOS symbol.\n",
    "        \n",
    "        In a bigram model, you need one bos and one eos; in a trigram model you need two bos and one eos, \n",
    "        and so on...\n",
    "        \"\"\"\n",
    "        \n",
    "        padded_sentences = []\n",
    "        for sentence in self.sentences:\n",
    "            padded_sentence = ['#bos#']*(self.ngram_size-1) + sentence + ['#eos#']\n",
    "            padded_sentences.append(padded_sentence)\n",
    "    \n",
    "        return padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates a language model object which can be trained and tested.\n",
    "    The language model has the following attributes:\n",
    "     - vocab: set of strings\n",
    "     - lam: float, indicating the constant to add to transition counts to smooth them (default to 1)\n",
    "     - ngram_size: int, the size of the ngrams\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n, vocab=None, smooth='Laplace', lam=1):\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.lam = lam\n",
    "        self.ngram_size = n\n",
    "        \n",
    "    def get_ngram(self, sentence, i):\n",
    "        \n",
    "        \"\"\"        \n",
    "        Takes in a list of string and an index, and returns the history and current \n",
    "        token of the appropriate size: the current token is the one at the provided \n",
    "        index, while the history consists of the n-1 previous tokens. If the ngram \n",
    "        size is 1, only the current token is returned.\n",
    "        \n",
    "        Example:\n",
    "        input sentence: ['bos', 'i', 'am', 'home', 'eos']\n",
    "        target index: 2\n",
    "        ngram size: 3\n",
    "        \n",
    "        ngram = ['bos', 'i', 'am']  \n",
    "        #from index 2-(3-1) = 0 to index i (the +1 is just because of how Python slices lists) \n",
    "        \n",
    "        history = ('bos', 'i')\n",
    "        target = 'am'\n",
    "        return (('bos', 'i'), 'am')\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.ngram_size == 1:\n",
    "            return sentence[i]\n",
    "        else:\n",
    "            ngram = sentence[i-(self.ngram_size-1):i+1]\n",
    "            history = tuple(ngram[:-1])\n",
    "            target = ngram[-1]\n",
    "            return (history, target)\n",
    "                    \n",
    "    def update_counts(self, corpus):\n",
    "        \n",
    "        \"\"\"        \n",
    "        Creates a transition matrix with counts in the form of a default dict mapping history\n",
    "        states to current states to the co-occurrence count (unless the ngram size is 1, in which\n",
    "        case the transition matrix is a simple counter mapping tokens to frequencies. \n",
    "        The ngram size of the corpus object has to be the same as the language model ngram size.\n",
    "        The input corpus (passed by providing the corpus object) is processed by extracting ngrams\n",
    "        of the chosen size and updating transition counts.\n",
    "        \n",
    "        This method creates three attributes for the language model object:\n",
    "         - counts: dict, described above\n",
    "         - vocab: set, containing all the tokens in the corpus\n",
    "         - vocab_size: int, indicating the number of tokens in the vocabulary\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.ngram_size != corpus.ngram_size:\n",
    "            raise ValueError(\"The corpus was pre-processed considering an ngram size of {} while the \"\n",
    "                             \"language model was created with an ngram size of {}. \\n\"\n",
    "                             \"Please choose the same ngram size for pre-processing the corpus and fitting \"\n",
    "                             \"the model.\".format(corpus.ngram_size, self.ngram_size))\n",
    "        \n",
    "        self.counts = defaultdict(dict) if self.ngram_size > 1 else Counter()\n",
    "        for sentence in corpus.sentences:\n",
    "            for idx in range(self.ngram_size-1, len(sentence)):\n",
    "                ngram = self.get_ngram(sentence, idx)\n",
    "                if self.ngram_size == 1:\n",
    "                    self.counts[ngram] += 1\n",
    "                else:\n",
    "                    # it's faster to try to do something and catch an exception than to use an if statement to check\n",
    "                    # whether a condition is met beforehand. The if is checked everytime, the exception is only catched\n",
    "                    # the first time, after that everything runs smoothly\n",
    "                    try:\n",
    "                        self.counts[ngram[0]][ngram[1]] += 1\n",
    "                    except KeyError:\n",
    "                        self.counts[ngram[0]][ngram[1]] = 1\n",
    "        \n",
    "        # first loop through the sentences in the corpus, than loop through each word in a sentence\n",
    "        self.vocab = {word for sentence in corpus.sentences for word in sentence}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "    \n",
    "    def get_unigram_probability(self, ngram):\n",
    "        \n",
    "        \"\"\"        \n",
    "        Compute the probability of a given unigram in the estimated language model using\n",
    "        Laplace smoothing (add k).\n",
    "        \"\"\"\n",
    "        \n",
    "        tot = sum(list(self.counts.values())) + (self.vocab_size*self.lam)\n",
    "        try:\n",
    "            ngram_count = self.counts[ngram] + self.lam\n",
    "        except KeyError:\n",
    "            ngram_count = self.lam\n",
    "            print(ngram_count, tot)\n",
    "        \n",
    "        return ngram_count/tot\n",
    "    \n",
    "    def get_bigram_probability(self, history, target):\n",
    "        \n",
    "        \"\"\"        \n",
    "        Compute the probability of a given bigram in the estimated language model using\n",
    "        Katz Backoff algorithm.\n",
    "        \n",
    "        The Katz Backoff algorithm work as follow for a bigram model:\n",
    "        \n",
    "        P_katz_2(history, target) = { \n",
    "            P_gt[r] / obsBi,      if r > 0\n",
    "            alpha(history) * P_mle[target],  if r == 0   \n",
    "                    }\n",
    "                    \n",
    "        Where P_gt is the bigram count (r) in the corpus subtracted by the mass probability for that\n",
    "        especificy count in the corpus; P_mle is the MLE for the target in the unigram model; and alpha\n",
    "        is the normalizing constant governing how the remaining probability mass should be distributed\n",
    "        to the unseen N-grams.\n",
    "        \n",
    "        - history: previous observed word in the corpus\n",
    "        - target: word that we want to predict the probability of occurance\n",
    "        \"\"\"\n",
    "        \n",
    "        # Calculate the total bigrams count for a especific history word\n",
    "        self.obsBi = np.sum(list(dict(self.counts[history]).values()))        \n",
    "        \n",
    "        # If it is an observed bigram: P_katz_2 = P_gt[r] / obsBi\n",
    "        try:\n",
    "            r = self.counts[history][target]\n",
    "            prob = self.probabilities[r] / self.obsBi\n",
    "\n",
    "        # If it is an unobserved bigram, then check unigrams: P_katz_2 = alpha * P_mle[target]\n",
    "        except KeyError:\n",
    "            \n",
    "            # alpha = (1 - sum( P_gt[count(history, w)] ) / obsBi) / (1 - sum( P_mle[w] ) / obsUni )\n",
    "            # w represents each observed word given the especific history\n",
    "            disc_uni = disc_bi = 0.0\n",
    "            for key, value in dict(self.counts[history]).items():\n",
    "                disc_bi += self.probabilities[value] \n",
    "                disc_uni += self.unicounts[key]\n",
    "\n",
    "            alpha = (1 - disc_bi / self.obsBi) / (1 - disc_uni / self.obsUni)\n",
    "\n",
    "            r = self.unicounts[target] \n",
    "            prob = alpha * r / self.obsUni\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def get_ngram_probability(self, history, target):\n",
    "        \n",
    "        \"\"\"        \n",
    "        Compute the conditional probability of the target token given the history, using \n",
    "        Laplace smoothing (add k).\n",
    "        \"\"\"\n",
    "                \n",
    "        try:\n",
    "            ngram_tot = np.sum(list(self.counts[history].values())) + (self.vocab_size*self.lam)\n",
    "            try:\n",
    "                transition_count = self.counts[history][target] + self.lam\n",
    "            except KeyError:\n",
    "                transition_count = self.lam\n",
    "        except KeyError:\n",
    "            transition_count = self.lam\n",
    "            ngram_tot = self.vocab_size*self.lam\n",
    "            \n",
    "        return transition_count/ngram_tot\n",
    "    \n",
    "    def perplexity(self, test_corpus):\n",
    "        \n",
    "        \"\"\"\n",
    "        Uses the estimated language model to process a corpus and computes the perplexity \n",
    "        of the language model over the corpus.\n",
    "        \"\"\"\n",
    "        \n",
    "        probs = []\n",
    "        for sentence in test_corpus.sentences:\n",
    "            for idx in range(self.ngram_size-1, len(sentence)):\n",
    "                ngram = self.get_ngram(sentence, idx)\n",
    "                if self.ngram_size == 1:\n",
    "                    probs.append(self.get_unigram_probability(ngram))\n",
    "                else:\n",
    "                    probs.append(self.get_bigram_probability(ngram[0], ngram[1]))\n",
    "        \n",
    "        entropy = np.log2(probs)\n",
    "        # this assertion makes sure that you retrieved valid probabilities, whose log must be <= 0\n",
    "        assert all(entropy <= 0)\n",
    "        \n",
    "        avg_entropy = -1 * (sum(entropy) / len(entropy))\n",
    "        \n",
    "        return pow(2.0, avg_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGT(object): \n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates the Simple Good-Turing (SGT) discount method of a given ngram model.\n",
    "    \n",
    "    - ngram_size: int, the size of the ngrams \n",
    "    - counts: dictionary, containing the ngram frequency counts\n",
    "    - k: int, Katz constant (which is suggested to be = 5)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.ngram_size = model.ngram_size\n",
    "        self.counts = model.counts\n",
    "        self.k = 5\n",
    "        \n",
    "        r, n, p0 = self.nCounts()\n",
    "        \n",
    "        z = self.Z_transform(r, n)            \n",
    "        \n",
    "        self.a, self.b = self.linear_regression(r, z)\n",
    "        \n",
    "        rStar = np.array(r, dtype=float)\n",
    "        for i in range(0, len(r)):\n",
    "            rStar[i] = self.GT_discount(r, i) if (i < self.k) else r[i] \n",
    "            \n",
    "        self.prob = self.find_prob(p0, r, n, rStar)\n",
    "    \n",
    "    def nCounts(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Creates a counts of counts dictionary that maps the number of occurances\n",
    "        of a given ngram frequency in the corpus.\n",
    "        \n",
    "        N[r] = sum(all ngram that occurs r times)\n",
    "        \n",
    "        returns:\n",
    "        - r_times: array, containing all the possible frequencies found in the ngram model\n",
    "        - n_ngram: array, containing the number of ngrams occurring r times\n",
    "        - p0: int, initial probability of unobserved items\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create a Nc dictionary based on the possible counts values\n",
    "        nc = dict()\n",
    "        for value in self.counts.values():\n",
    "            for r in dict(value).values():\n",
    "                try:\n",
    "                    nc[r] += 1\n",
    "                except KeyError:\n",
    "                    nc[r] = 1\n",
    "\n",
    "        sorted_nc = dict(sorted(nc.items()))\n",
    "        r_times = np.array(list(sorted_nc.keys()), dtype=int)\n",
    "        n_ngrams = np.array(list(sorted_nc.values()), dtype=int)\n",
    "        p0 = n_ngrams[0] / np.inner(r_times, n_ngrams)\n",
    "                            \n",
    "        return r_times, n_ngrams, p0\n",
    "\n",
    "    def Z_transform(self, r, n):\n",
    "        \n",
    "        \"\"\"\n",
    "        To handle the case that most of the Nr are zero for large r, which is a \n",
    "        necessary value when calculating r∗. Instead of depending on the raw count Nr\n",
    "        of each non-zero frequency, these non-zero values are averaged and replaced by:\n",
    "            Zr = Nr / 0.5 * (t−q)\n",
    "        where t and q are, respectively, the successor and the predecessor\n",
    "        of a given r value that were observed in the ngram model.\n",
    "        for the first r index, q = 0 and for the last index, t = 2*r - q.\n",
    "        \n",
    "        returns:\n",
    "        - Z: array, containing the estimated Z values given count of counts arrays (r, n)\n",
    "        \"\"\"\n",
    "        \n",
    "        Z = dict()\n",
    "\n",
    "        # First:  Zr = Nr/0.5*(t-q) with t = 2 and q = 0\n",
    "        Z[r[0]] = 2 * n[0] / r[1]\n",
    "        \n",
    "        # Last: Zr = Nr/(r-q) with t = (2*r - q) and q = r-1 \n",
    "        Z[r[-1]] = n[-1] / (r[-1] - r[-2]) \n",
    "\n",
    "        # General case: Nr/0.5*(t-q)\n",
    "        for idx in range(1, len(r) - 1):\n",
    "            Z[r[idx]] = 2 * n[idx] / (r[idx+1] - r[idx-1])\n",
    "        \n",
    "        Z = dict(sorted(Z.items()))\n",
    "        \n",
    "        return np.array(list(Z.values()), dtype=float)\n",
    "\n",
    "    def linear_regression(self, r, Z):\n",
    "        \n",
    "        \"\"\"\n",
    "        Linear regression method to find the appropriated parameters (a, b)\n",
    "        for the equation:\n",
    "            log(Zr) = a + b * log(r)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Working with logarithm values due to the risk of underflow\n",
    "        l = len(r)\n",
    "        logr = np.log(r)\n",
    "        logZ = np.log(Z)\n",
    "            \n",
    "        # Find the mean values for log(z):y and log(r):x\n",
    "        meanX = np.sum(logr)/l\n",
    "        meanY = np.sum(logZ)/l\n",
    "        \n",
    "        # Iterate through the arrays to find the values' deviation from the mean\n",
    "        xy = x2 = 0\n",
    "        for i in range(0, l):\n",
    "            xy += (logr[i] - meanX) * (logZ[i] - meanY)\n",
    "            x2 += (logr[i] - meanX)**2\n",
    "    \n",
    "        # Find the logistic regression parameters\n",
    "        b = xy/x2\n",
    "        a = meanY - b*meanX\n",
    "                \n",
    "        return a, b\n",
    "    \n",
    "    def Z_smoothed(self, i):\n",
    "        \n",
    "        \"\"\"\n",
    "        Aply Zr[i] = exp(a + b*log(r[i])) to find the smoothed values for Nr(now Zr)\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.exp(self.a + self.b * np.log(i))\n",
    "    \n",
    "    def GT_discount(self, r, i):\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculate new ngram frequency count (r*) estimate for lower values (< k) of r\n",
    "        so these values can lose some probability mass that went to the unobserved ngrams.        \n",
    "        \"\"\"\n",
    "        \n",
    "        # r* = ((r+1) * Zr+1/Zr - r * (k+1) * Zk+1/Z0) / (1 - (k+1) * Zk+1/Z0)\n",
    "        \n",
    "        rStar = (((r[i] + 1) * self.Z_smoothed(r[i]+1) / self.Z_smoothed(r[i])) - \n",
    "                 (r[i] * (r[self.k] + 1) * self.Z_smoothed(r[self.k]+1) / self.Z_smoothed(r[0])))\n",
    "        \n",
    "        rStar /= 1 - (r[self.k] + 1) * self.Z_smoothed(r[self.k]+1) / self.Z_smoothed(r[0])\n",
    "        \n",
    "        return rStar\n",
    "\n",
    "    def find_prob(self, p0, r, n, rStar):\n",
    "        \n",
    "        \"\"\"\n",
    "        Find SGT discount probabilities and already subtract them from the\n",
    "        original r count to optimize the Katz Backoff computation\n",
    "        \n",
    "        p = (1 - p0) * (rStar * n) / total\n",
    "        \n",
    "        (rStar * n) = counts of an especific r value\n",
    "        total = sum of all counts in the model\n",
    "        \n",
    "        returns:\n",
    "        - p: dictionary, containing the original r frequencies subtracted by the SGT discount probability\n",
    "        \"\"\"\n",
    "        \n",
    "        p = Counter()\n",
    "\n",
    "        total = np.inner(n, rStar)\n",
    "        for idx in range(0, len(n)):\n",
    "            p[r[idx]] = (1 - p0) * rStar[idx] * n[idx] / total\n",
    "            p[r[idx]] = r[idx] - p[r[idx]]\n",
    "                \n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fold(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates a fold from the original corpus\n",
    "    \"\"\"\n",
    "    \n",
    "    def freq_distr(self):    \n",
    "        return Counter([word for sentence in self.sentences for word in sentence])\n",
    "    \n",
    "    def filter_words(self):\n",
    "        filtered_sentences = []\n",
    "        for sentence in self.sentences:\n",
    "            filtered_sentence = []\n",
    "            for word in sentence:\n",
    "                # check if the word occurs in the vocabulary\n",
    "                filtered_sentence.append(word if word in self.vocab else 'UNK')\n",
    "                \n",
    "            if len(filtered_sentence) > 1:\n",
    "                # make sure that the sentence contains more than 1 token\n",
    "                filtered_sentences.append(filtered_sentence)\n",
    "    \n",
    "        return filtered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_corpus(corpus, n):\n",
    "    \n",
    "    \"\"\"\n",
    "    Split the corpus into n folds to separate the training from the test set using CV \n",
    "    \"\"\"\n",
    "    \n",
    "    step = round(len(corpus.sentences)/n)\n",
    "    \n",
    "    nfolds = []\n",
    "    for i in range(0, n):\n",
    "        new = Fold()\n",
    "        new.sentences = corpus.sentences[i*step : (i+1)*step - 1]\n",
    "        new.ngram_size = corpus.ngram_size\n",
    "        new.frequencies = new.freq_distr()\n",
    "        nfolds.append(new)\n",
    "        \n",
    "    return nfolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example code to run a bigram model with Katz backoff algorithm and Good-Turing discounting.\n",
    "\n",
    "uni_corpus = Corpus(train_path, 1, 10, bos_eos=True, vocab=None)\n",
    "bi_corpus = Corpus(train_path, 2, 10, bos_eos=True, vocab=None)\n",
    "\n",
    "uni_model = LM(1)\n",
    "bi_model = LM(2)\n",
    "\n",
    "uni_model.update_counts(uni_corpus)\n",
    "bi_model.update_counts(bi_corpus)\n",
    "\n",
    "# Calculate Simple Goood-Turing discount\n",
    "bi_sgt = SGT(bi_model)\n",
    "\n",
    "# Incorporate SGT probabilities in the bigram model\n",
    "bi_model.probabilities = bi_sgt.prob\n",
    "\n",
    "# N-gram preparation: calculate previously the total observed unigrams and attach unigram counts to the bigram model\n",
    "bi_model.obsUni = np.sum(list(uni_model.counts.values()))\n",
    "bi_model.unicounts = uni_model.counts\n",
    "\n",
    "# to ensure consistency, the test corpus is filtered using the vocabulary of the trained language model\n",
    "test_corpus = Corpus(test_path, 2, 10, bos_eos=True, vocab=bi_model.vocab)\n",
    "print(bi_model.perplexity(test_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Best case scenario: use the train_corpus as test_corpus to analise max likelihood\n",
    "n = 2\n",
    "corpus = Corpus(train_path, 10, n, bos_eos=True, vocab=None)\n",
    "bigram_model = LM(n, lam=0.001)\n",
    "bigram_model.update_counts(corpus)\n",
    "\n",
    "bigram_model.perplexity(corpus)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# example code to run a unigram model with add 0.001 smoothing. Tokens with a frequency count lower than 10\n",
    "# are replaced with the UNK string\n",
    "n = 1\n",
    "train_corpus = Corpus(train_path, 10, n, bos_eos=True, vocab=None)\n",
    "unigram_model = LM(n, lam=0.001)\n",
    "unigram_model.update_counts(train_corpus)\n",
    "\n",
    "# to ensure consistency, the test corpus is filtered using the vocabulary of the trained language model\n",
    "test_corpus = Corpus(test_path, None, n, bos_eos=True, vocab=unigram_model.vocab)\n",
    "unigram_model.perplexity(test_corpus)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# example code to run a bigram model with add 0.001 smoothing. The same frequency threshold is applied.\n",
    "n = 2\n",
    "train_corpus = Corpus(train_path, 10, n, bos_eos=True, vocab=None)\n",
    "bigram_model = LM(n, lam=0.001)\n",
    "bigram_model.update_counts(train_corpus)\n",
    "\n",
    "# to ensure consistency, the test corpus is filtered using the vocabulary of the trained language model\n",
    "test_corpus = Corpus(test_path, None, n, bos_eos=True, vocab=bigram_model.vocab)\n",
    "bigram_model.perplexity(test_corpus)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# example code to run a bigram model with add 0.001 smoothing.\n",
    "n = 2\n",
    "corpus = Corpus(train_path, 10, bos_eos=True, vocab=None)\n",
    "\n",
    "if corpus.bos_eos:\n",
    "    corpus.sentences = corpus.add_bos_eos(2)\n",
    "\n",
    "shuffle(corpus.sentences)\n",
    "nfolds = split_corpus(corpus, 10)\n",
    "\n",
    "bigram_model = LM(n, lam=0.001)\n",
    "for i in range(0, 9):\n",
    "    bigram_model.update_counts(nfolds[i])\n",
    "    \n",
    "# Check vocabulary in the test_corpus\n",
    "nfolds[9].vocab = bigram_model.vocab\n",
    "nfolds[9].sentences = nfolds[9].filter_words()\n",
    "\n",
    "print(bigram_model.perplexity(nfolds[9]))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
